{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nIN_COLAB = False\nIN_KAGGLE = False\ntry:\n    if 'google.colab' in str(get_ipython()):\n        IN_COLAB = True\nexcept NameError:\n    pass\nif not IN_COLAB:\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n        IN_KAGGLE = True\n\n\n# Determine the path to the repository based on the environment\nif IN_COLAB:\n    path_to_repo = '/content/vlPFC_Visual_Geometry'\nelif IN_KAGGLE:\n    path_to_repo = '/kaggle/working/vlPFC_Visual_Geometry'\nelse:\n    # Environment where the .py file is in the root of the repo\n    path_to_repo = '.'\n\nif IN_COLAB or IN_KAGGLE:\n    !pip install -q timm open_clip_torch git+https://github.com/openai/CLIP.git \\\n  --extra-index-url https://download.pytorch.org/whl/cu118\n# Only clone if not already present\nif not os.path.exists(path_to_repo): \n    !git clone https://github.com/jobellet/vlPFC_Visual_Geometry.git\nsys.path.append(path_to_repo)\nsys.path.append(os.path.join(path_to_repo, 'utils')) # Add the utils directory to sys.path\n\n\nfrom utils.extract_and_download_data import download_files, unzip\nfrom utils.image_processing import m_pathway_filter_gaussian\n\n\nimport shutil\nimport pickle\nimport warnings\nfrom pathlib import Path\nfrom glob import glob\nimport cv2\nimport numpy as np\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom multiprocessing import cpu_count\nfrom tqdm import tqdm\nimport torch\nimport torchvision\nimport timm\nimport clip\nimport open_clip\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torchvision.transforms import InterpolationMode\nimport types\n# -----------------------------------------------------------------------------\n#  Setup and data download\n# -----------------------------------------------------------------------------\ndef setup_and_download():\n    \"\"\"\n    Clones the git repository, prompts for a private link token, and downloads\n    the required data files.\n    \"\"\"\n    \n    from utils.extract_and_download_data import download_files, unzip\n\n    private_link = input(\"Please enter your private link token: \")\n\n    files_to_download = [\n        \"high_variation_stimuli.zip\",\n        \"inpainted_images.zip\"\n    ]\n    download_files(path_to_repo,files_to_download, private_link)\n\n    unzip(\"downloads/high_variation_stimuli.zip\", \"\")\n    unzip(\"downloads/inpainted_images.zip\", \"inpainted_images\")\n\n\n\n# -----------------------------------------------------------------------------\n#  Main image filtering loop\n# -----------------------------------------------------------------------------\ndef filter_images():\n    \"\"\"\n    Applies a low-pass filter to the high-variation stimuli images.\n    \"\"\"\n    stimulus_folder = 'high_variation_stimuli'\n    output_folder   = 'high_variation_stimuli_lowpass'\n    os.makedirs(output_folder, exist_ok=True)\n    \n    img_files = sorted(glob(stimulus_folder+\"/*.png\"))\n    for img_path in tqdm(img_files):\n        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n        if img is None:\n            continue\n        lp  = m_pathway_filter_gaussian(img)\n        # OpenCV expects 8‑ or 32‑bit depths for colour conversion → cast\n        if lp.dtype != np.uint8:\n            lp = np.clip(lp, 0, 255).astype(np.uint8)\n        lp_rgb = cv2.cvtColor(lp, cv2.COLOR_GRAY2RGB)\n        cv2.imwrite(os.path.join(output_folder,os.path.split(img_path)[1]), lp_rgb)\n    \n\n\n# -----------------------------------------------------------------------------\n#  Feature extraction\n# -----------------------------------------------------------------------------\ndef extract_features():\n    \"\"\"\n    Extracts penultimate layer features from various deep neural networks.\n    \"\"\"\n    os.system('pip install -q timm open_clip_torch git+https://github.com/openai/CLIP.git --extra-index-url https://download.pytorch.org/whl/cu118')\n\n    GPU  = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if GPU else \"cpu\")\n    print(\"Using device:\", device)\n\n    IN_ROOT   = Path.cwd()\n    OUT_ROOT  = Path(\"deepNetFeatures\");  OUT_ROOT.mkdir(exist_ok=True)\n\n    CONDITIONS = {\n        \"high_variation_original\":  \"high_variation_stimuli\",\n        \"high_variation_lowpass\":   \"high_variation_stimuli_lowpass\",\n        \"inpainted_images_original\":  \"inpainted_images\",\n    }\n\n    CKPT = Path(\"_tmp_ckpt\");  CKPT.mkdir(exist_ok=True)\n    os.environ.update(TORCH_HOME=str(CKPT), XDG_CACHE_HOME=str(CKPT))\n\n    SUP_TIMM = {\n        \"ViT_base_patch16_224\"            : (\"vit_base_patch16_224\",            \"head_drop\"),\n        \"DeiT_small_distilled_patch16_224\": (\"deit_small_distilled_patch16_224\",\"head\"),\n        \"Swin_base_patch4_window7_224\"    : (\"swin_base_patch4_window7_224\",    \"head.fc\"),\n        \"ConvNeXt_base_in22ft1k\"          : (\"convnext_base_in22ft1k\",          \"head.drop\"),\n        \"EfficientNet_B0\"                 : (\"efficientnet_b0\",                 \"global_pool.flatten\"),\n        \"MobileNetV3_small_100\"           : (\"mobilenetv3_small_100\",           \"flatten\"),\n        \"ViT_large_patch16_224\"           : (\"vit_large_patch16_224\",           \"head_drop\"),\n        \"DeiT3_small_patch16_224\"         : (\"deit3_small_patch16_224\",         \"head_drop\"),\n        \"Swin_large_patch4_window7_224\"   : (\"swin_large_patch4_window7_224\",   \"head.fc\"),\n        \"ConvNeXt_tiny_in22ft1k\"          : (\"convnext_tiny_in22ft1k\",          \"head.drop\"),\n        \"MobileNetV3_large_100\"           : (\"mobilenetv3_large_100\",           \"flatten\"),\n    }\n\n    SUP_TV = {\n        \"ResNet50\"      : (torchvision.models.resnet50,      \"avgpool\"),\n        \"ResNet101\"     : (torchvision.models.resnet101,     \"avgpool\"),\n        \"Inception_v3\"  : (torchvision.models.inception_v3,  \"dropout\"),\n    }\n\n    CLIP_MODELS = {\n        \"CLIP_ViT-B/32\": (\"ViT-B/32\", \"visual.ln_post\"),\n        \"CLIP_RN50\"    : (\"RN50\",     \"visual.attnpool\"),\n    }\n\n    OPENCLIP = {\n        \"OpenCLIP_ViT-B/32_openai\"  : (\"ViT-B-32\", \"openai\",             \"visual.ln_post\"),\n        \"OpenCLIP_ViT-B/32_laion2b\" : (\"ViT-B-32\", \"laion2b_s34b_b79k\",  \"visual.ln_post\"),\n        \"OpenCLIP_RN50_openai\"      : (\"RN50\",     \"openai\",             \"visual.attnpool\"),\n        \"OpenCLIP_RN101_openai\"     : (\"RN101\",    \"openai\",             \"visual.attnpool\"),\n    }\n\n    DINO_TIMM = {\n        \"ViT_S16_DINO\": (\"vit_small_patch16_224\", \"head_drop\"),\n        \"ViT_B16_DINO\": (\"vit_base_patch16_224\",  \"head_drop\"),\n    }\n    DINO_HUB = {\n        \"DINO_ResNet50\": (\"facebookresearch/dino:main\", \"dino_resnet50\"),\n    }\n\n    MODELS = {}\n    for k,v in SUP_TIMM.items():   MODELS[k] = dict(fam=\"sup_timm\", arch=v[0], pen=v[1])\n    for k,v in SUP_TV.items():     MODELS[k] = dict(fam=\"sup_tv\",   ctor=v[0], pen=v[1])\n    for k,v in CLIP_MODELS.items():MODELS[k] = dict(fam=\"clip\",     arch=v[0], pen=v[1])\n    for k,v in OPENCLIP.items():   MODELS[k] = dict(fam=\"openclip\", arch=v[0], weights=v[1], pen=v[2])\n    for k,v in DINO_TIMM.items():  MODELS[k] = dict(fam=\"dino_timm\",arch=v[0], pen=v[1])\n    for k,v in DINO_HUB.items():   MODELS[k] = dict(fam=\"dino_hub\", repo=v[0], entry=v[1])\n\n    def safe_name(name: str) -> str:\n        return name.replace(\"/\", \"_\")\n\n    TX_STD = T.Compose([T.Resize(256), T.CenterCrop(224),\n                        T.ToTensor(),\n                        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n\n    def load_batch(paths, tx, dev):\n        imgs = [tx(Image.open(p).convert(\"RGB\")) for p in paths]\n        return torch.stack(imgs).to(dev)\n\n    def attach_hook(model, layer):\n        buf = {}\n        for n,m in model.named_modules():\n            if n==layer:\n                m.register_forward_hook(lambda _,__,o: buf.setdefault(\"x\", o))\n                return buf\n        raise RuntimeError(f\"layer {layer} not found\")\n\n    def make_fwd(model, store):\n        def fn(x):\n            store.clear()\n            _ = model(x)\n            return store[\"x\"]\n        return fn\n\n    BATCH = 32 if GPU else 4\n\n    for nick, spec in MODELS.items():\n        out_paths = [\n            OUT_ROOT / f\"{safe_name(nick)}_features_{cond}.pkl\"\n            for cond in CONDITIONS\n        ]\n        if all(p.exists() for p in out_paths):\n            print(f\"All pickle files for {nick} exist; skipping model load.\")\n            continue\n\n        fam = spec[\"fam\"]\n        run_dev = device\n        print(f\"\\n=== {nick}  ({fam}) on {run_dev} ===\")\n\n        try:\n            if fam == \"sup_timm\":\n                mdl = timm.create_model(spec[\"arch\"], pretrained=True).to(run_dev).eval()\n                buf = attach_hook(mdl, spec[\"pen\"]); fwd = make_fwd(mdl, buf); preprocess = TX_STD\n            elif fam == \"sup_tv\":\n                mdl = spec[\"ctor\"](pretrained=True).to(run_dev).eval()\n                buf = attach_hook(mdl, spec[\"pen\"]); fwd = make_fwd(mdl, buf); preprocess = TX_STD\n            elif fam == \"clip\":\n                mdl, preprocess = clip.load(spec[\"arch\"], device=run_dev, jit=False)\n                mdl.eval(); fwd = lambda x: mdl.encode_image(x)\n            elif fam == \"openclip\":\n                mdl, _, preprocess = open_clip.create_model_and_transforms(\n                    spec[\"arch\"], pretrained=spec[\"weights\"], device=run_dev)\n                mdl.eval(); fwd = lambda x: mdl.encode_image(x)\n            elif fam == \"dino_timm\":\n                mdl = timm.create_model(spec[\"arch\"], pretrained=True,\n                                        pretrained_cfg_overlay=dict(tag=\"dino\")).to(run_dev).eval()\n                fwd = mdl; preprocess = TX_STD\n            elif fam == \"dino_hub\":\n                utils_mod = sys.modules.get(\"utils\", types.ModuleType(\"utils\"))\n                def trunc_normal_(tensor, mean=0., std=1.):\n                    return torch.nn.init.trunc_normal_(tensor, mean=mean, std=std)\n                utils_mod.trunc_normal_ = trunc_normal_\n                sys.modules[\"utils\"] = utils_mod\n                mdl = torch.hub.load(spec[\"repo\"], spec[\"entry\"])\n                mdl.to(run_dev).eval()\n                fwd = mdl; preprocess = TX_STD\n            else:\n                raise RuntimeError(\"unexpected family\")\n\n        except Exception as e:\n            warnings.warn(f\"  !! could not load {nick}: {e}\")\n            shutil.rmtree(CKPT, ignore_errors=True); CKPT.mkdir(exist_ok=True)\n            continue\n\n        for cond, folder_name in CONDITIONS.items():\n            out_pkl = OUT_ROOT / f\"{safe_name(nick)}_features_{cond}.pkl\"\n            if os.path.exists(out_pkl):\n                print(f\"{out_pkl} already exists. Skipping ...\")\n                continue\n\n            folder = IN_ROOT / folder_name\n            if not folder.exists():\n                print(f\"Folder {folder} does not exist, skipping condition {cond}\")\n                continue\n\n            files  = sorted([p for p in folder.iterdir()\n                             if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\"}])\n            feats, names = [], []\n\n            for i in tqdm(range(0,len(files),BATCH), desc=f\"{nick} | {cond}\", leave=False):\n                batch_paths = files[i:i+BATCH]\n                x = load_batch(batch_paths, preprocess, run_dev)\n                with torch.no_grad():\n                    out = fwd(x).detach().cpu()\n                feats.append(out)\n                names += [p.stem for p in batch_paths]\n\n            if not feats:\n                print(f\"No features extracted for {nick} | {cond}\")\n                continue\n\n            feats = torch.cat(feats).numpy()\n\n            with open(out_pkl,\"wb\") as fh:\n                pickle.dump({\"penultimate\":feats, \"image_names\":names}, fh, protocol=pickle.HIGHEST_PROTOCOL)\n            print(f\"  saved {out_pkl.name:45s} {feats.shape}\")\n\n        del mdl; torch.cuda.empty_cache()\n        shutil.rmtree(CKPT, ignore_errors=True); CKPT.mkdir(exist_ok=True)\n        print(\"  (cache cleared)\")\n\n    output_zip_name = 'deepNetFeatures'\n    shutil.make_archive(output_zip_name, 'zip', root_dir=OUT_ROOT, base_dir='.')\n    print(f\"\\nAll .pkl files zipped into {output_zip_name}.zip\")\n\n\nif __name__ == '__main__':\n    setup_and_download()\n    filter_images()\n    extract_features()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:50:53.315471Z","iopub.execute_input":"2025-07-04T09:50:53.315813Z","iopub.status.idle":"2025-07-04T10:15:17.191877Z","shell.execute_reply.started":"2025-07-04T09:50:53.315781Z","shell.execute_reply":"2025-07-04T10:15:17.191081Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Please enter your private link token:  69ed62d8d9d6464acc0a\n"},{"name":"stdout","text":"downloads/high_variation_stimuli.zip already exists.\ndownloads/inpainted_images.zip already exists.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2560/2560 [00:45<00:00, 55.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n\n=== ViT_base_patch16_224  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5213029fc045dc83bdb48e41eddb11"}},"metadata":{}},{"name":"stderr","text":"                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_base_patch16_224_features_high_variation_original.pkl (2560, 768)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                              \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_base_patch16_224_features_high_variation_lowpass.pkl (2560, 768)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_base_patch16_224_features_inpainted_images_original.pkl (640, 768)\n  (cache cleared)\n\n=== DeiT_small_distilled_patch16_224  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/89.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdcce9809c7445688c234c72bebf5b3f"}},"metadata":{}},{"name":"stderr","text":"                                                                                                           \r","output_type":"stream"},{"name":"stdout","text":"  saved DeiT_small_distilled_patch16_224_features_high_variation_original.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  saved DeiT_small_distilled_patch16_224_features_high_variation_lowpass.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                             \r","output_type":"stream"},{"name":"stdout","text":"  saved DeiT_small_distilled_patch16_224_features_inpainted_images_original.pkl (640, 1000)\n  (cache cleared)\n\n=== Swin_base_patch4_window7_224  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4efbe99e08d24d7fa21be6bbbcd55892"}},"metadata":{}},{"name":"stderr","text":"                                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved Swin_base_patch4_window7_224_features_high_variation_original.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                      \r","output_type":"stream"},{"name":"stdout","text":"  saved Swin_base_patch4_window7_224_features_high_variation_lowpass.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name convnext_base_in22ft1k to current convnext_base.fb_in22k_ft_in1k.\n  model = create_fn(\n","output_type":"stream"},{"name":"stdout","text":"  saved Swin_base_patch4_window7_224_features_inpainted_images_original.pkl (640, 1000)\n  (cache cleared)\n\n=== ConvNeXt_base_in22ft1k  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f4d574074884fdaaec2f87a8565fe62"}},"metadata":{}},{"name":"stderr","text":"                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  saved ConvNeXt_base_in22ft1k_features_high_variation_original.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"  saved ConvNeXt_base_in22ft1k_features_high_variation_lowpass.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                   ","output_type":"stream"},{"name":"stdout","text":"  saved ConvNeXt_base_in22ft1k_features_inpainted_images_original.pkl (640, 1024)\n  (cache cleared)\n\n=== EfficientNet_B0  (sup_timm) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6e743966e5f4ed2b91ecda925a89fde"}},"metadata":{}},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  saved EfficientNet_B0_features_high_variation_original.pkl (2560, 1280)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"  saved EfficientNet_B0_features_high_variation_lowpass.pkl (2560, 1280)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                            ","output_type":"stream"},{"name":"stdout","text":"  saved EfficientNet_B0_features_inpainted_images_original.pkl (640, 1280)\n  (cache cleared)\n\n=== MobileNetV3_small_100  (sup_timm) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/10.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a1a47d58cb44878d0b2ed8403d0b20"}},"metadata":{}},{"name":"stderr","text":"                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"  saved MobileNetV3_small_100_features_high_variation_original.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"  saved MobileNetV3_small_100_features_high_variation_lowpass.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"  saved MobileNetV3_small_100_features_inpainted_images_original.pkl (640, 1024)\n  (cache cleared)\n\n=== ViT_large_patch16_224  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed882c922c043498e35d26b9e63a0cd"}},"metadata":{}},{"name":"stderr","text":"                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_large_patch16_224_features_high_variation_original.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_large_patch16_224_features_high_variation_lowpass.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_large_patch16_224_features_inpainted_images_original.pkl (640, 1024)\n  (cache cleared)\n\n=== DeiT3_small_patch16_224  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/88.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82205247e47b46fabb0c5659824c5e25"}},"metadata":{}},{"name":"stderr","text":"                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"  saved DeiT3_small_patch16_224_features_high_variation_original.pkl (2560, 384)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  saved DeiT3_small_patch16_224_features_high_variation_lowpass.pkl (2560, 384)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"  saved DeiT3_small_patch16_224_features_inpainted_images_original.pkl (640, 384)\n  (cache cleared)\n\n=== Swin_large_patch4_window7_224  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/788M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f1262924f24da7932be465c434e645"}},"metadata":{}},{"name":"stderr","text":"                                                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  saved Swin_large_patch4_window7_224_features_high_variation_original.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved Swin_large_patch4_window7_224_features_high_variation_lowpass.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py:126: UserWarning: Mapping deprecated model name convnext_tiny_in22ft1k to current convnext_tiny.fb_in22k_ft_in1k.\n  model = create_fn(\n","output_type":"stream"},{"name":"stdout","text":"  saved Swin_large_patch4_window7_224_features_inpainted_images_original.pkl (640, 1000)\n  (cache cleared)\n\n=== ConvNeXt_tiny_in22ft1k  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17057b728ce6440baf8c9f058491eb15"}},"metadata":{}},{"name":"stderr","text":"                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  saved ConvNeXt_tiny_in22ft1k_features_high_variation_original.pkl (2560, 768)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"  saved ConvNeXt_tiny_in22ft1k_features_high_variation_lowpass.pkl (2560, 768)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"  saved ConvNeXt_tiny_in22ft1k_features_inpainted_images_original.pkl (640, 768)\n  (cache cleared)\n\n=== MobileNetV3_large_100  (sup_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f4a4d5ad549432190943261f9799683"}},"metadata":{}},{"name":"stderr","text":"                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"  saved MobileNetV3_large_100_features_high_variation_original.pkl (2560, 1280)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"  saved MobileNetV3_large_100_features_high_variation_lowpass.pkl (2560, 1280)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"  saved MobileNetV3_large_100_features_inpainted_images_original.pkl (640, 1280)\n  (cache cleared)\n\n=== ResNet50  (sup_tv) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to _tmp_ckpt/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 172MB/s]\n                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"  saved ResNet50_features_high_variation_original.pkl (2560, 2048, 1, 1)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"  saved ResNet50_features_high_variation_lowpass.pkl  (2560, 2048, 1, 1)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"  saved ResNet50_features_inpainted_images_original.pkl (640, 2048, 1, 1)\n  (cache cleared)\n\n=== ResNet101  (sup_tv) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to _tmp_ckpt/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:00<00:00, 188MB/s] \n                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"  saved ResNet101_features_high_variation_original.pkl (2560, 2048, 1, 1)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"  saved ResNet101_features_high_variation_lowpass.pkl (2560, 2048, 1, 1)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"  saved ResNet101_features_inpainted_images_original.pkl (640, 2048, 1, 1)\n  (cache cleared)\n\n=== Inception_v3  (sup_tv) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to _tmp_ckpt/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 185MB/s] \n                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved Inception_v3_features_high_variation_original.pkl (2560, 2048, 1, 1)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                      \r","output_type":"stream"},{"name":"stdout","text":"  saved Inception_v3_features_high_variation_lowpass.pkl (2560, 2048, 1, 1)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"  saved Inception_v3_features_inpainted_images_original.pkl (640, 2048, 1, 1)\n  (cache cleared)\n\n=== CLIP_ViT-B/32  (clip) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 64.6MiB/s]\n                                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  saved CLIP_ViT-B_32_features_high_variation_original.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved CLIP_ViT-B_32_features_high_variation_lowpass.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  saved CLIP_ViT-B_32_features_inpainted_images_original.pkl (640, 512)\n  (cache cleared)\n\n=== CLIP_RN50  (clip) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"100%|███████████████████████████████████████| 244M/244M [00:04<00:00, 52.3MiB/s]\n                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"  saved CLIP_RN50_features_high_variation_original.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"  saved CLIP_RN50_features_high_variation_lowpass.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                      \r","output_type":"stream"},{"name":"stdout","text":"  saved CLIP_RN50_features_inpainted_images_original.pkl (640, 1024)\n  (cache cleared)\n\n=== OpenCLIP_ViT-B/32_openai  (openclip) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6347d0171b0b46888506f0b3409e9846"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n  warnings.warn(\n                                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_ViT-B_32_openai_features_high_variation_original.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_ViT-B_32_openai_features_high_variation_lowpass.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                     \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_ViT-B_32_openai_features_inpainted_images_original.pkl (640, 512)\n  (cache cleared)\n\n=== OpenCLIP_ViT-B/32_laion2b  (openclip) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70657509f045496eb3573c5597fbe4db"}},"metadata":{}},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_ViT-B_32_laion2b_features_high_variation_original.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_ViT-B_32_laion2b_features_high_variation_lowpass.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                      \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_ViT-B_32_laion2b_features_inpainted_images_original.pkl (640, 512)\n  (cache cleared)\n\n=== OpenCLIP_RN50_openai  (openclip) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/408M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af9fccdcd90452c9906ec96f9237a9c"}},"metadata":{}},{"name":"stderr","text":"                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_RN50_openai_features_high_variation_original.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                              \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_RN50_openai_features_high_variation_lowpass.pkl (2560, 1024)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_RN50_openai_features_inpainted_images_original.pkl (640, 1024)\n  (cache cleared)\n\n=== OpenCLIP_RN101_openai  (openclip) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"open_clip_model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be6904ff88aa48e0bfff996948ec604d"}},"metadata":{}},{"name":"stderr","text":"                                                                                                \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_RN101_openai_features_high_variation_original.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                               \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_RN101_openai_features_high_variation_lowpass.pkl (2560, 512)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"  saved OpenCLIP_RN101_openai_features_inpainted_images_original.pkl (640, 512)\n  (cache cleared)\n\n=== ViT_S16_DINO  (dino_timm) on cuda ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/88.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8922dc9d46164795bc53546279f30219"}},"metadata":{}},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_S16_DINO_features_high_variation_original.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                      \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_S16_DINO_features_high_variation_lowpass.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_S16_DINO_features_inpainted_images_original.pkl (640, 1000)\n  (cache cleared)\n\n=== ViT_B16_DINO  (dino_timm) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_B16_DINO_features_high_variation_original.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                      \r","output_type":"stream"},{"name":"stdout","text":"  saved ViT_B16_DINO_features_high_variation_lowpass.pkl (2560, 1000)\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to _tmp_ckpt/hub/main.zip\n","output_type":"stream"},{"name":"stdout","text":"  saved ViT_B16_DINO_features_inpainted_images_original.pkl (640, 1000)\n  (cache cleared)\n\n=== DINO_ResNet50  (dino_hub) on cuda ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nDownloading: \"https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth\" to _tmp_ckpt/hub/checkpoints/dino_resnet50_pretrain.pth\n100%|██████████| 90.0M/90.0M [00:00<00:00, 302MB/s]\n                                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  saved DINO_ResNet50_features_high_variation_original.pkl (2560, 2048)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  saved DINO_ResNet50_features_high_variation_lowpass.pkl (2560, 2048)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"  saved DINO_ResNet50_features_inpainted_images_original.pkl (640, 2048)\n  (cache cleared)\n\nAll .pkl files zipped into deepNetFeatures.zip\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}