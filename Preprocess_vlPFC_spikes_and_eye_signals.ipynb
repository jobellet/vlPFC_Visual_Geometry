{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUOs51xkVx5yiHKa50MNaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobellet/vlPFC_Visual_Geometry/blob/main/Preprocess_vlPFC_spikes_and_eye_signals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# This notebook reproduce the preprocessing steps used to align the spike times and eye signal to the time of stimulus onset\n",
        "---\n",
        "For the notebook to download the data from the Figshare repository prior to acceptance of the manuscript you need to insert the private link token mentioned in the \"Code availability\" section of the manuscript.\n",
        "---"
      ],
      "metadata": {
        "id": "2VBTBgIQX7fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "private_link = input('Enter the private link token:')"
      ],
      "metadata": {
        "id": "5EJuREg6X2I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import repository and download data"
      ],
      "metadata": {
        "id": "zH-T1DLCGlih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVfdv2NrpX1U",
        "outputId": "f5e9440e-04a6-49d6-d54e-cad6ef161539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vlPFC_Visual_Geometry'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 14 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14/14), 4.50 KiB | 768.00 KiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Downloading downloads/continuous_signals_spikes_and_events.zip...\n"
          ]
        }
      ],
      "source": [
        "files_to_download = ['continuous_signals_spikes_and_events.zip']\n",
        "import os\n",
        "import sys\n",
        "IN_COLAB = False\n",
        "IN_KAGGLE = False\n",
        "try:\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        IN_COLAB = True\n",
        "except NameError:\n",
        "    pass\n",
        "if not IN_COLAB:\n",
        "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n",
        "        IN_KAGGLE = True\n",
        "\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    path_to_repo = '/content/vlPFC_Visual_Geometry'\n",
        "elif IN_KAGGLE:\n",
        "    path_to_repo = '/kaggle/working/vlPFC_Visual_Geometry'\n",
        "else:\n",
        "    path_to_repo = 'vlPFC_Visual_Geometry/'\n",
        "# Only clone if not already present\n",
        "if not os.path.exists(path_to_repo):\n",
        "    !git clone https://github.com/jobellet/vlPFC_Visual_Geometry.git\n",
        "sys.path.append(path_to_repo)\n",
        "\n",
        "\n",
        "from utils.extract_and_download_data import download_files, unzip\n",
        "\n",
        "# Load the file-code mapping\n",
        "\n",
        "download_files(path_to_repo, files_to_download, private_link=private_link)\n",
        "zip_path =  os.path.join('downloads', 'continuous_signals_spikes_and_events.zip')\n",
        "unzip(zip_path,'raw_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViuYwi_wzPwr"
      },
      "source": [
        "### Align Spikes, Eye Traces, and Pupil Data to Stimulus Onset Times  \n",
        "*(0 → 200 ms relative to stimulus onse)*\n",
        "\n",
        "This script loops through each recording-day `.mat` file and generates three aligned outputs per day:\n",
        "\n",
        "1. **Neural spike counts** — `YYYYMMDD_stim_aligned_count.npy`  \n",
        "   *shape (nTrials × 96 channels × 20 bins, 10 ms/bin; 0–200 ms)*  \n",
        "2. **Calibrated eye-position traces** — `YYYYMMDD_eye_pos.npy`  \n",
        "   *shape (nTrials × 400 samples × 2 axes, 0.2 s at 2 kHz; 0–200 ms)*  \n",
        "3. **Baseline-corrected pupil traces** — `YYYYMMDD_pupil_aligned.npy`  \n",
        "   *shape (nTrials × 400 samples, 0.2 s at 2 kHz; 0–200 ms)*  \n",
        "\n",
        "#### Processing Pipeline\n",
        "\n",
        "| Stage | Details |\n",
        "|-------|---------|\n",
        "| **Input discovery** | Scans the working directory for files matching `20*.mat`; skips any that already have all three output `.npy` files. |\n",
        "| **Data loading** | Reads spike times, eye traces (`eye_pos_x`, `eye_pos_y`), pupil size (`eye_pupil`), and stimulus onsets (`stimTime`). |\n",
        "| **Stimulus-time correction** | Shifts each onset **½ video frame earlier** (−8.33 ms at 60 Hz) to compensate for photodiode delay, then converts to sample indices (30 kHz). |\n",
        "| **Eye-position alignment** | Extracts a **200 ms window (0 → +200 ms)** @ 2 kHz, subtracts the median of the neighbouring 15 trials to compensate the slow measurement drift, and maps raw voltages to ° of visual angle via a fixed 2 × 3 affine transform. |\n",
        "| **Spike binning** | Counts spikes in 10 ms bins (**20 bins spanning 0 → +200 ms**) for each of 96 channels using Numba-accelerated search-sorted binning. |\n",
        "| **Pupil alignment** | Extracts the same 200 ms window from `eye_pupil` and subtracts the median of the first 40 samples. |\n",
        "| **Saving** | Writes the three outputs with date-encoded filenames so downstream scripts can locate them easily. |\n",
        "| **Parallel execution** | Uses `multiprocessing.Pool` to process multiple recording days in parallel. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn_S9u61J5B-",
        "outputId": "18b3eb88-e86f-4ad2-d290-32845687574a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201102.mat\n",
            "Processing raw_data/20201023.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201023: 100%|██████████| 3963/3963 [00:12<00:00, 327.45it/s] \n",
            "Spikes 20201102:  61%|██████    | 4182/6903 [00:12<00:02, 1177.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201020.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201102: 100%|██████████| 6903/6903 [00:14<00:00, 475.82it/s] \n",
            "Spikes 20201020:  47%|████▋     | 959/2058 [00:01<00:01, 683.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201104.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201020: 100%|██████████| 2058/2058 [00:02<00:00, 795.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201103.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201104: 100%|██████████| 9530/9530 [00:07<00:00, 1230.58it/s]\n",
            "Spikes 20201103: 100%|██████████| 8282/8282 [00:06<00:00, 1219.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201030.mat\n",
            "Processing raw_data/20201022.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201022: 100%|██████████| 3946/3946 [00:04<00:00, 914.52it/s] \n",
            "Spikes 20201030:  64%|██████▍   | 3943/6136 [00:04<00:01, 1231.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201106.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201030: 100%|██████████| 6136/6136 [00:05<00:00, 1046.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201110.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201106: 100%|██████████| 5731/5731 [00:04<00:00, 1240.68it/s]\n",
            "Spikes 20201110:  29%|██▊       | 2415/8457 [00:01<00:05, 1200.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201021.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201110: 100%|██████████| 8457/8457 [00:09<00:00, 931.39it/s]\n",
            "Spikes 20201021:  81%|████████▏ | 1922/2365 [00:02<00:00, 718.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201028.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201021: 100%|██████████| 2365/2365 [00:03<00:00, 712.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201029.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201028: 100%|██████████| 4793/4793 [00:05<00:00, 826.79it/s] \n",
            "Spikes 20201029:  74%|███████▎  | 4716/6405 [00:06<00:01, 1163.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201109.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201029: 100%|██████████| 6405/6405 [00:07<00:00, 889.43it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201027.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201109: 100%|██████████| 6433/6433 [00:11<00:00, 556.48it/s]\n",
            "Spikes 20201027:  74%|███████▍  | 4608/6213 [00:07<00:02, 661.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201112.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201027: 100%|██████████| 6213/6213 [00:09<00:00, 655.64it/s] \n",
            "Spikes 20201112: 100%|██████████| 9873/9873 [00:09<00:00, 1073.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing raw_data/20201026.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Spikes 20201026: 100%|██████████| 6718/6718 [00:02<00:00, 2270.47it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import numba\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from scipy.io import loadmat\n",
        "from scipy.signal import medfilt as movmedian   # simple 1-D median filter\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- geometry helper --------------------------------------------------\n",
        "def transform_points(points):\n",
        "    \"\"\"Affine transform from voltage space to visual-angle space (hard-coded).\"\"\"\n",
        "    T_hard_coded = np.array([[ 6.86341798e-03,  3.92195313e-04, -2.22044166e-16],\n",
        "                             [-1.16678106e-03,  5.64761251e-03, -4.03638942e-16]])\n",
        "    points_h = np.hstack([points, np.ones((points.shape[0], 1))])  # hom. coords\n",
        "    return (points_h @ T_hard_coded.T)[:, :2]\n",
        "\n",
        "# ---------- eye-trace alignment ----------------------------------------------\n",
        "def calibrate_and_align_eye_pos(eye_pos_x, eye_pos_y, stimulus_times):\n",
        "    \"\"\"\n",
        "    Aligns eye position traces in the window 0–200 ms relative to stimulus onset.\n",
        "    Returns: eye_pos (nTrials × 400 × 2), in_fix mask (nTrials × 400).\n",
        "    \"\"\"\n",
        "    rate = 2000          # Hz\n",
        "    post = int(0.2 * rate)   # 400 samples, 200 ms\n",
        "    baseline_len = 40     # first 20 ms for baseline subtraction\n",
        "\n",
        "    stim_idx = (stimulus_times * rate).astype(np.int32)[:, None]\n",
        "    indices  = stim_idx + np.arange(0, post)             # 0 → +199 ms\n",
        "\n",
        "    X = eye_pos_x[0, indices]\n",
        "    Y = eye_pos_y[0, indices]\n",
        "\n",
        "    # median baseline (20 ms) per trial\n",
        "    X_med = movmedian(np.mean(X[:, :baseline_len], axis=1), 15).reshape(-1, 1)\n",
        "    Y_med = movmedian(np.mean(Y[:, :baseline_len], axis=1), 15).reshape(-1, 1)\n",
        "\n",
        "    X_adj = X - X_med\n",
        "    Y_adj = Y - Y_med\n",
        "\n",
        "    pts   = np.column_stack((X_adj.ravel(), Y_adj.ravel()))\n",
        "    out   = transform_points(pts).reshape(X_adj.shape[0], post, 2)\n",
        "\n",
        "    in_fix = (np.linalg.norm(out, axis=2) < 2)  # 2° radius fixation window\n",
        "    return out, in_fix\n",
        "\n",
        "# ---------- pupil alignment ---------------------------------------------------\n",
        "def align_pupil_data(eye_pupil, stimulus_times):\n",
        "    \"\"\"\n",
        "    Aligns pupil traces to stimulus onset, keeping only 0–200 ms.\n",
        "    Returns: pupil_aligned (nTrials × 400).\n",
        "    \"\"\"\n",
        "    rate = 2000\n",
        "    post = int(0.2 * rate)   # 400 samples\n",
        "    baseline_len = 40        # first 20 ms\n",
        "\n",
        "    stim_idx = (stimulus_times * rate).astype(np.int32)[:, None]\n",
        "    indices  = stim_idx + np.arange(0, post)\n",
        "\n",
        "    pupil = eye_pupil[0, indices]\n",
        "    baseline = np.median(pupil[:, :baseline_len], axis=1, keepdims=True)\n",
        "    return pupil - baseline\n",
        "\n",
        "# ---------- spike-count binning ----------------------------------------------\n",
        "TIME_WINDOW = 10 * 30        # 300 samples = 10 ms @ 30 kHz\n",
        "NUM_BINS    = 20             # 0–200 ms\n",
        "@numba.njit\n",
        "def compute_channel_counts(spike_times, edges):\n",
        "    return np.diff(np.searchsorted(spike_times, edges)).astype(np.int32)\n",
        "\n",
        "def get_spike_count(spike, t):\n",
        "    \"\"\"\n",
        "    Compute 10 ms spike counts from 0–200 ms after stimulus onset.\n",
        "    Returns: (96 × 20) int32 array.\n",
        "    \"\"\"\n",
        "    edges = np.arange(t, t + (NUM_BINS + 1) * TIME_WINDOW, TIME_WINDOW)\n",
        "    counts = np.zeros((96, NUM_BINS), dtype=np.int32)\n",
        "    for ch in range(96):\n",
        "        st = spike[0][ch][0, :]   # 1-D sorted spike times (samples)\n",
        "        counts[ch, :] = compute_channel_counts(st, edges)\n",
        "    return counts\n",
        "\n",
        "# ---------- per-file processing ----------------------------------------------\n",
        "def process_file(mat_file):\n",
        "    print(f\"Processing {mat_file}\")\n",
        "    base = os.path.splitext(os.path.basename(mat_file))[0]\n",
        "    f_spk, f_eye, f_pup = [f\"aligned_data/{base}_{suffix}.npy\" for suffix in\n",
        "                           (\"stim_aligned_count\", \"eye_pos\", \"pupil_aligned\")]\n",
        "    if all(os.path.exists(f) for f in (f_spk, f_eye, f_pup)):\n",
        "        print(\"  → outputs already exist, skipping.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        m = loadmat(mat_file)\n",
        "        stimTime = m[\"stimTime\"][0]       # seconds\n",
        "        spike    = m[\"spike\"]\n",
        "        eye_x    = m[\"eye_pos_x\"]\n",
        "        eye_y    = m[\"eye_pos_y\"]\n",
        "        eye_pup  = m[\"eye_pupil\"]\n",
        "\n",
        "        # Constant photodiode correction (–8.33 ms) to prevent underestimating response delay in vlPFC due screen rolling shutter effect\n",
        "        stim_corr = stimTime - (1 / 60) / 2\n",
        "\n",
        "        eye_pos, _ = calibrate_and_align_eye_pos(eye_x, eye_y, stim_corr)\n",
        "        pupil      = align_pupil_data(eye_pup, stim_corr)\n",
        "\n",
        "        stim_counts = np.zeros((len(stim_corr), 96, NUM_BINS), dtype=np.int32)\n",
        "        stim_corr_samples = (stim_corr * 30000).astype(np.int32)\n",
        "\n",
        "        for s, t in tqdm(enumerate(stim_corr_samples),\n",
        "                         total=len(stim_corr_samples),\n",
        "                         desc=f\"Spikes {base}\"):\n",
        "            stim_counts[s] = get_spike_count(spike, t)\n",
        "\n",
        "        np.save(f_spk, stim_counts)\n",
        "        np.save(f_eye, eye_pos)\n",
        "        np.save(f_pup, pupil)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {mat_file}: {e}\")\n",
        "\n",
        "# ---------- main entry-point --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs('aligned_data/', exist_ok=True)\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        pool.map(process_file, glob(os.path.join('raw_data',\"20*.mat\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcU6kDCq0btS"
      },
      "source": [
        "### Aggregate Daily Outputs & Down-sample Eye/Pupil Signals to match spike count 10 ms binning\n",
        "\n",
        "This script collates the per-day `.npy` files created above into single, experiment-wide arrays and a tidy metadata table:\n",
        "\n",
        "1. **Spike counts** → `aggregated_spike_counts.npy`  \n",
        "   *shape (nTotalTrials × 96 × 20)*  \n",
        "2. **Eye position** (down-sampled to 100 Hz) → `aggregated_eye_pos.npy`  \n",
        "   *shape (nTotalTrials × 20 × 2)*  \n",
        "3. **Pupil traces** (down-sampled to 100 Hz) → `aggregated_pupil.npy`  \n",
        "   *shape (nTotalTrials × 20)*  \n",
        "4. **Trial-wise metadata CSV** → `metadata.csv`  \n",
        "   Columns: global trial index, recording date, `stimId`, original `stimTime`.\n",
        "\n",
        "#### Key Steps\n",
        "\n",
        "1. **File matching** – Identifies every `.mat` file beginning with `20`. For each, it expects the three aligned outputs generated by Script 1; files lacking them are skipped with a warning.  \n",
        "2. **Down-sampling** – Eye and pupil traces (2 kHz) are reduced to 100 Hz (20×) using `downsample_array`, preserving the **0 → 200 ms** window as **20 samples**.  \n",
        "3. **Data stacking** – Trial-level arrays are appended to running lists; after all days are processed, `np.concatenate` merges them along the trial axis.  \n",
        "4. **Metadata assembly** – A `pandas` DataFrame stores per-trial identifiers and is written to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dae78c9-7ca8-4e0f-f364-f7c11b254737",
        "id": "x9lz3KMZ0btS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: 20201102\n",
            "Processing file: 20201104\n",
            "Processing file: 20201023\n",
            "Processing file: 20201020\n",
            "Processing file: 20201103\n",
            "Processing file: 20201022\n",
            "Processing file: 20201030\n",
            "Processing file: 20201110\n",
            "Processing file: 20201106\n",
            "Processing file: 20201021\n",
            "Processing file: 20201028\n",
            "Processing file: 20201109\n",
            "Processing file: 20201029\n",
            "Processing file: 20201027\n",
            "Processing file: 20201112\n",
            "Processing file: 20201026\n",
            "Data aggregation complete!\n",
            "Total trials aggregated: 97806\n",
            "Metadata shape: (97806, 4)\n",
            "Spike count array shape: (97806, 96, 20)\n",
            "Eye position array shape: (97806, 20, 2)\n",
            "Pupil array shape: (97806, 20)\n"
          ]
        }
      ],
      "source": [
        "def downsample_array(array, factor, axis):\n",
        "    \"\"\"\n",
        "    Downsample a NumPy array along a specified axis by a given factor.\n",
        "\n",
        "    Parameters:\n",
        "    - array: np.ndarray, input data of any shape.\n",
        "    - factor: int, downsampling factor (keep every `factor`-th element). For instance factor = 2 halfs the sampling rate.\n",
        "    - axis: int, axis along which to downsample.\n",
        "\n",
        "    Returns:\n",
        "    - downsampled_array: np.ndarray, array downsampled along the specified axis.\n",
        "    \"\"\"\n",
        "    # Build a slicing object: slice(None) for all axes, except step-slice on the target axis\n",
        "    slicer = [slice(None)] * array.ndim\n",
        "    slicer[axis] = slice(None, None, factor)\n",
        "    return array[tuple(slicer)]\n",
        "\n",
        "\n",
        "# List all .mat files starting with \"20\" in the current directory\n",
        "mat_files = glob(os.path.join('raw_data','20*.mat'))\n",
        "\n",
        "# Prepare lists to accumulate data from all files\n",
        "all_metadata = []\n",
        "all_spike_counts = []\n",
        "all_eye_pos = []\n",
        "all_pupil = []\n",
        "\n",
        "# Running counter for total trials\n",
        "total_trials = 0\n",
        "\n",
        "for mat_file in mat_files:\n",
        "    base = os.path.splitext(os.path.basename(mat_file))[0]\n",
        "    print(f\"Processing file: {base}\")\n",
        "\n",
        "    spike_file = f\"aligned_data/{base}_stim_aligned_count.npy\"\n",
        "    eye_file   = f\"aligned_data/{base}_eye_pos.npy\"\n",
        "    pupil_file = f\"aligned_data/{base}_pupil_aligned.npy\"\n",
        "\n",
        "    if not (os.path.exists(mat_file) and os.path.exists(spike_file)\n",
        "            and os.path.exists(eye_file) and os.path.exists(pupil_file)):\n",
        "        print(f\"Skipping file {base}: required files not found.\")\n",
        "        continue\n",
        "\n",
        "    mat_data = loadmat(mat_file)\n",
        "    stimId   = mat_data['stimId'][0] if 'stimId' in mat_data else None\n",
        "    stimTime = mat_data['stimTime'][0] if 'stimTime' in mat_data else None\n",
        "\n",
        "    recording_date = base\n",
        "\n",
        "    stim_aligned_count = np.load(spike_file)  # shape: (nTrials, 96, 20)\n",
        "    eye_pos            = np.load(eye_file)    # shape: (nTrials, 400, 2)\n",
        "    pupil_aligned      = np.load(pupil_file)  # shape: (nTrials, 400)\n",
        "\n",
        "    nTrials = stim_aligned_count.shape[0]\n",
        "\n",
        "    # Downsample from 2 kHz → 100 Hz (factor 20): 400 → 20 samples\n",
        "    eye_pos_downsampled   = downsample_array(eye_pos, factor=20, axis=1)    # (nTrials, 20, 2)\n",
        "    pupil_downsampled     = downsample_array(pupil_aligned, factor=20, axis=1)  # (nTrials, 20)\n",
        "\n",
        "    for i in range(nTrials):\n",
        "        all_metadata.append({\n",
        "            'trial_index_global': total_trials + i,\n",
        "            'recording_date': recording_date,\n",
        "            'stimId': stimId[i] if stimId is not None else None,\n",
        "            'time_of_stimulus': stimTime[i] if stimTime is not None else None\n",
        "        })\n",
        "\n",
        "    all_spike_counts.append(stim_aligned_count.astype(np.int8))\n",
        "    all_eye_pos.append(eye_pos_downsampled.astype(np.float32))\n",
        "    all_pupil.append(pupil_downsampled.astype(np.float32))\n",
        "\n",
        "    total_trials += nTrials\n",
        "\n",
        "# Final concatenation\n",
        "all_spike_counts = np.concatenate(all_spike_counts, axis=0)  # (nTotalTrials, 96, 20)\n",
        "all_eye_pos      = np.concatenate(all_eye_pos, axis=0)       # (nTotalTrials, 20, 2)\n",
        "all_pupil        = np.concatenate(all_pupil, axis=0)         # (nTotalTrials, 20)\n",
        "\n",
        "metadata_df = pd.DataFrame(all_metadata)\n",
        "\n",
        "# Save outputs\n",
        "metadata_df.to_csv(\"recording_metadata.csv\", index=False)\n",
        "np.save(\"aggregated_spike_counts.npy\", all_spike_counts)\n",
        "np.save(\"aggregated_eye_pos.npy\", all_eye_pos)\n",
        "np.save(\"aggregated_pupil.npy\", all_pupil)\n",
        "\n",
        "# Sanity print\n",
        "print(\"Data aggregation complete!\")\n",
        "print(f\"Total trials aggregated: {total_trials}\")\n",
        "print(f\"Metadata shape: {metadata_df.shape}\")\n",
        "print(f\"Spike count array shape: {all_spike_counts.shape}\")\n",
        "print(f\"Eye position array shape: {all_eye_pos.shape}\")\n",
        "print(f\"Pupil array shape: {all_pupil.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o-jVgsXz0sn"
      },
      "source": [
        "### Split Odd- vs. Even-Day Trials & Stimulus-Average Responses\n",
        "\n",
        "This step takes the aggregated trial-level arrays and produces\n",
        "stimulus-wise averages for **odd** and **even recording days** separately.\n",
        "\n",
        "#### Workflow\n",
        "\n",
        "| Stage | Action | Detail |\n",
        "|-------|--------|--------|\n",
        "| **1. Load aggregated data** | `recording_metadata.csv`, `aggregated_spike_counts.npy`, `aggregated_eye_pos.npy`, `aggregated_pupil.npy` | Shapes: (nTrials × 96 × 20), (nTrials × 20 × 2), (nTrials × 20) |\n",
        "| **2. Temporal sort** | Sort trials by `recording_date` + `time_of_stimulus` | Guarantees chronological order within a day |\n",
        "| **3. Session index** | Map each `recording_date` → integer index | Needed to tag even vs odd days |\n",
        "| **4. Trial-exclusion masks** | *a)* Drop any stimulus that repeats within the **previous two trials** of the same day<br>*b)* Require fixation radius < 2° for the **entire 0–200 ms window** | Reduces adaptation effects and eye-movement artefacts |\n",
        "| **5. Session-specific outlier filter** | Within each day, discard trials whose **mean firing rate** falls outside the 0.1-th–99.9-th percentile range | Removes electrical artefacts or silent blocks |\n",
        "| **6. Session baseline shift** | For every session, subtract the channel-wise mean firing rate (averaged over time bins) before averaging | Aligns days with different baselines |\n",
        "| **7. Odd/Even split & averaging** | For each `stimId`, compute the mean of baseline-shifted trials **separately** for odd-indexed sessions and even-indexed sessions | For each channel, outliers are prevented by clipping rates between 1st and 99th percentiles |\n",
        "| **8. Eye & pupil means** | Simultaneously average eye-position (20 samples @ 100 Hz) and pupil traces for odd vs even groups | For control analysis |\n",
        "| **9. Save outputs** | ```\n",
        "Spike_count_even_sessions.npy   # (nStim × 96 × 20)\n",
        "Spike_count_odd_sessions.npy\n",
        "Mean_eye_position_even_sessions.npy   # (nStim × 20 × 2)\n",
        "Mean_eye_position_odd_sessions.npy\n",
        "Mean_pupil_size_even_sessions.npy     # (nStim × 20)\n",
        "Mean_pupil_size_odd_sessions.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TvVf5u0KBXDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f15395-a65d-4274-d613-e7c101b42986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Computing session-based shifted averages per stimulus, excluding bad trials...\n",
            "Computing mean eye position and pupil size per stimulus for even and odd sessions...\n",
            "Saving outputs...\n",
            "Done! Session-split shifted arrays and mean signals saved.\n"
          ]
        }
      ],
      "source": [
        "metadata_csv = \"recording_metadata.csv\"\n",
        "spike_file = \"aggregated_spike_counts.npy\"\n",
        "eye_file = \"aggregated_eye_pos.npy\"\n",
        "pupil_file = \"aggregated_pupil.npy\"\n",
        "n_stim_ids = 3200\n",
        "output_prefix = \"\"\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# 1) Load data\n",
        "metadata_df = pd.read_csv(metadata_csv)\n",
        "all_spike_counts = np.load(spike_file)   # shape: (nTrials, 96, 20)\n",
        "all_eye_pos = np.load(eye_file)          # shape: (nTrials, 20, 2)\n",
        "all_pupil = np.load(pupil_file)          # shape: (nTrials, 20)\n",
        "\n",
        "# 2) Sort trials by [recording_date, time_of_stimulus]\n",
        "if \"time_of_stimulus\" not in metadata_df.columns:\n",
        "    raise ValueError(\"Metadata must have a 'time_of_stimulus' column for sorting.\")\n",
        "\n",
        "metadata_df[\"sort_key\"] = (\n",
        "    metadata_df[\"recording_date\"].astype(str) + \"_\" +\n",
        "    metadata_df[\"time_of_stimulus\"].astype(str)\n",
        ")\n",
        "sorted_indices = np.argsort(metadata_df[\"sort_key\"].values)\n",
        "\n",
        "metadata_df = metadata_df.iloc[sorted_indices].reset_index(drop=True)\n",
        "all_spike_counts = all_spike_counts[sorted_indices]\n",
        "all_eye_pos = all_eye_pos[sorted_indices]\n",
        "all_pupil = all_pupil[sorted_indices]\n",
        "\n",
        "# 3) Compute day indices (sessions) for each trial\n",
        "unique_days = np.sort(metadata_df[\"recording_date\"].unique())\n",
        "day_index_map = {day: i for i, day in enumerate(unique_days)}\n",
        "day_indices = metadata_df[\"recording_date\"].map(day_index_map).values\n",
        "\n",
        "# 4) Identify non-repeating stimuli\n",
        "nTrials = len(metadata_df)\n",
        "non_repeating_stimuli = np.ones(nTrials, dtype=bool)\n",
        "stim_ids_series = metadata_df[\"stimId\"].values\n",
        "\n",
        "for i in range(1, nTrials):\n",
        "    start_idx = max(0, i - 2)\n",
        "    if np.any(stim_ids_series[start_idx:i] == stim_ids_series[i]):\n",
        "        non_repeating_stimuli[i] = False\n",
        "\n",
        "# 5) Identify in-fixation-window trials (within 2° radius during all 20 samples)\n",
        "radius = np.sqrt(all_eye_pos[..., 0]**2 + all_eye_pos[..., 1]**2)  # (nTrials, 20)\n",
        "in_fix_window_trials = np.mean(radius < 2, axis=1) == 1\n",
        "\n",
        "# 6) Apply validity mask and filter outliers by session\n",
        "valid_mask = non_repeating_stimuli & in_fix_window_trials\n",
        "sessions = np.unique(day_indices)\n",
        "\n",
        "for s in sessions:\n",
        "    session_indices = np.where(day_indices == s)[0]\n",
        "    valid_session_indices = session_indices[valid_mask[session_indices]]\n",
        "    if valid_session_indices.size > 0:\n",
        "        trial_mean_fr = np.mean(all_spike_counts[valid_session_indices], axis=(1, 2))\n",
        "        lo = np.percentile(trial_mean_fr, 0.1)\n",
        "        hi = np.percentile(trial_mean_fr, 99.9)\n",
        "        condition = (trial_mean_fr > lo) & (trial_mean_fr < hi)\n",
        "        valid_mask[valid_session_indices] = condition\n",
        "\n",
        "# 7) Compute per-session mean for baseline shift\n",
        "session_means = {}\n",
        "for s in sessions:\n",
        "    valid_trials = np.where((day_indices == s) & valid_mask)[0]\n",
        "    if valid_trials.size > 0:\n",
        "        session_means[s] = np.mean(all_spike_counts[valid_trials], axis=(0, 2))  # (96,)\n",
        "    else:\n",
        "        session_means[s] = np.zeros(96)\n",
        "\n",
        "# 8) Initialize output arrays for spike counts\n",
        "Spike_count_even = np.full((n_stim_ids, 96, 20), np.nan, dtype=np.float32)\n",
        "Spike_count_odd  = np.full((n_stim_ids, 96, 20), np.nan, dtype=np.float32)\n",
        "\n",
        "print(\"Computing session-based shifted averages per stimulus, excluding bad trials...\")\n",
        "\n",
        "# Loop over each stimulus\n",
        "for stim_id in range(n_stim_ids):\n",
        "    stim_indices = np.where((stim_ids_series == stim_id) & valid_mask)[0]\n",
        "    if stim_indices.size == 0:\n",
        "        continue\n",
        "\n",
        "    even_indices = stim_indices[(day_indices[stim_indices] % 2) == 0]\n",
        "    odd_indices  = stim_indices[(day_indices[stim_indices] % 2) == 1]\n",
        "\n",
        "    shifted_trials_even = [\n",
        "        all_spike_counts[idx] - session_means[day_indices[idx]][:, None]\n",
        "        for idx in even_indices\n",
        "    ]\n",
        "    if shifted_trials_even:\n",
        "        Spike_count_even[stim_id] = np.mean(shifted_trials_even, axis=0)\n",
        "\n",
        "    shifted_trials_odd = [\n",
        "        all_spike_counts[idx] - session_means[day_indices[idx]][:, None]\n",
        "        for idx in odd_indices\n",
        "    ]\n",
        "    if shifted_trials_odd:\n",
        "        Spike_count_odd[stim_id] = np.mean(shifted_trials_odd, axis=0)\n",
        "\n",
        "# 9) Clip spike counts channel-wise between 1st and 99th percentiles\n",
        "def clip_spike_count(spike_count):\n",
        "    rate_channel = spike_count.swapaxes(0, 1).reshape(96, -1)\n",
        "    low, high = np.percentile(rate_channel, [1, 99], axis=1)\n",
        "    clipped = np.clip(rate_channel, low[:, None], high[:, None])\n",
        "    return clipped.reshape(spike_count.swapaxes(0, 1).shape).swapaxes(0, 1)\n",
        "\n",
        "Spike_count_even = clip_spike_count(Spike_count_even)\n",
        "Spike_count_odd = clip_spike_count(Spike_count_odd)\n",
        "\n",
        "# 10) Initialize output arrays for eye and pupil signals\n",
        "Mean_eye_position_even = np.full((n_stim_ids, 20, 2), np.nan, dtype=np.float32)\n",
        "Mean_eye_position_odd  = np.full((n_stim_ids, 20, 2), np.nan, dtype=np.float32)\n",
        "Mean_pupil_size_even   = np.full((n_stim_ids, 20), np.nan, dtype=np.float32)\n",
        "Mean_pupil_size_odd    = np.full((n_stim_ids, 20), np.nan, dtype=np.float32)\n",
        "\n",
        "print(\"Computing mean eye position and pupil size per stimulus for even and odd sessions...\")\n",
        "\n",
        "# Loop over each stimulus ID for eye/pupil averaging\n",
        "for stim_id in range(n_stim_ids):\n",
        "    stim_indices = np.where((stim_ids_series == stim_id) & valid_mask)[0]\n",
        "    if stim_indices.size == 0:\n",
        "        continue\n",
        "\n",
        "    even_indices = stim_indices[(day_indices[stim_indices] % 2) == 0]\n",
        "    odd_indices  = stim_indices[(day_indices[stim_indices] % 2) == 1]\n",
        "\n",
        "    if even_indices.size > 0:\n",
        "        Mean_eye_position_even[stim_id] = np.mean(all_eye_pos[even_indices], axis=0)\n",
        "        Mean_pupil_size_even[stim_id] = np.mean(all_pupil[even_indices], axis=0)\n",
        "    if odd_indices.size > 0:\n",
        "        Mean_eye_position_odd[stim_id] = np.mean(all_eye_pos[odd_indices], axis=0)\n",
        "        Mean_pupil_size_odd[stim_id] = np.mean(all_pupil[odd_indices], axis=0)\n",
        "\n",
        "# 11) Save the results\n",
        "print(\"Saving outputs...\")\n",
        "np.save(f\"{output_prefix}Spike_count_even_sessions.npy\", Spike_count_even)\n",
        "np.save(f\"{output_prefix}Spike_count_odd_sessions.npy\", Spike_count_odd)\n",
        "np.save(f\"{output_prefix}Mean_eye_position_even_sessions.npy\", Mean_eye_position_even)\n",
        "np.save(f\"{output_prefix}Mean_eye_position_odd_sessions.npy\", Mean_eye_position_odd)\n",
        "np.save(f\"{output_prefix}Mean_pupil_size_even_sessions.npy\", Mean_pupil_size_even)\n",
        "np.save(f\"{output_prefix}Mean_pupil_size_odd_sessions.npy\", Mean_pupil_size_odd)\n",
        "\n",
        "print(\"Done! Session-split shifted arrays and mean signals saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Package Final Outputs into a ZIP Archive\n",
        "\n",
        "This cell creates a compressed archive containing the final, downstream files:\n",
        "\n",
        "- `aggregated_spike_counts.npy`  \n",
        "- `aggregated_eye_pos.npy`  \n",
        "- `aggregated_pupil.npy`  \n",
        "- `recording_metadata.csv`  \n",
        "- `Spike_count_even_sessions.npy`  \n",
        "- `Spike_count_odd_sessions.npy`  \n",
        "- `Mean_eye_position_even_sessions.npy`  \n",
        "- `Mean_eye_position_odd_sessions.npy`  \n",
        "- `Mean_pupil_size_even_sessions.npy`  \n",
        "- `Mean_pupil_size_odd_sessions.npy`  "
      ],
      "metadata": {
        "id": "ZPirtgTlBJNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output archive name\n",
        "output_zip = \"final_outputs.zip\"\n",
        "\n",
        "# Regex patterns for files to include\n",
        "include_patterns = [\n",
        "    r\"^aggregated_spike_counts\\.npy$\",\n",
        "    r\"^aggregated_eye_pos\\.npy$\",\n",
        "    r\"^aggregated_pupil\\.npy$\",\n",
        "    r\"^recording_metadata\\.csv$\",\n",
        "    r\"^Spike_count_even_sessions\\.npy$\",\n",
        "    r\"^Spike_count_odd_sessions\\.npy$\",\n",
        "    r\"^Mean_eye_position_even_sessions\\.npy$\",\n",
        "    r\"^Mean_eye_position_odd_sessions\\.npy$\",\n",
        "    r\"^Mean_pupil_size_even_sessions\\.npy$\",\n",
        "    r\"^Mean_pupil_size_odd_sessions\\.npy$\"\n",
        "]\n",
        "\n",
        "# Compile regexes\n",
        "compiled = [re.compile(p) for p in include_patterns]\n",
        "\n",
        "with zipfile.ZipFile(output_zip, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    for fname in os.listdir(\".\"):\n",
        "        # Only include files (no directories)\n",
        "        if not os.path.isfile(fname):\n",
        "            continue\n",
        "        # Check if filename matches any include pattern\n",
        "        if any(rx.match(fname) for rx in compiled):\n",
        "            print(f\"Adding {fname}\")\n",
        "            zf.write(fname, arcname=fname)\n",
        "\n",
        "print(f\"\\nCreated archive with final outputs: {output_zip}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8uZ3Y9yYjpI",
        "outputId": "2c7706bd-059f-4853-ae29-b19801e1c4a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding Mean_eye_position_even_sessions.npy\n",
            "Adding recording_metadata.csv\n",
            "Adding Spike_count_even_sessions.npy\n",
            "Adding Mean_pupil_size_even_sessions.npy\n",
            "Adding Mean_eye_position_odd_sessions.npy\n",
            "Adding Spike_count_odd_sessions.npy\n",
            "Adding aggregated_eye_pos.npy\n",
            "Adding aggregated_spike_counts.npy\n",
            "Adding Mean_pupil_size_odd_sessions.npy\n",
            "Adding aggregated_pupil.npy\n",
            "\n",
            "Created archive with final outputs: final_outputs.zip\n"
          ]
        }
      ]
    }
  ]
}